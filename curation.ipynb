{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goals of this research:\n",
    "- Method to curate data, where curation means:\n",
    "    - Fix data errors, including wrong values or wrong data types.\n",
    "    - Fill out missing data.\n",
    "- Able to support user-entered rules?\n",
    "\n",
    "Files and settings:\n",
    "- Dropbox Folder: ⁨Dropbox⁩/⁨01.INVESTIGACION⁩/⁨[2019]⁩/⁨2019-09-17/AUTOMATED METADATA CURATION⁩\n",
    "- Tool: PyCharm\n",
    "- GitHub repo: cedar-experiments-curation\n",
    "- Workspace: workspace folder in the current execution folder\n",
    "- ARM Jupyter notebook: https://nbviewer.jupyter.org/github/metadatacenter/cedar-experiments-valuerecommender2019/blob/master/ValueRecommenderEvaluation.ipynb \n",
    "- cedar-valuerecommender-server: branch 'experiments-curation'\n",
    "\n",
    "\n",
    "# _Automatic metadata curation_\n",
    "\n",
    "[Stanford Center for Biomedical Informatics Research](https://bmir.stanford.edu/), 1265 Welch Road, Stanford University School of Medicine, Stanford, CA 94305-5479, USA\n",
    "\n",
    "\\* Correspondence: marcosmr@stanford.edu\n",
    "\n",
    "## Purpose of this document\n",
    "\n",
    "This document is a [Jupyter notebook](http://jupyter.org/) that describes ...\n",
    "\n",
    "The scripts used to generate the results and figures in the paper are in the [scripts folder](./scripts). The results generated when running the code cells in this notebook will be saved to a local `workspace` folder.\n",
    "\n",
    "\n",
    "## Table of contents\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"s0\"></a>Viewing and running this notebook\n",
    "\n",
    "GitHub will automatically generate a static online view of this notebook. However, current GitHub's rendering does not support some features, such as the anchor links that connect the 'Table of contents' to the different sections. A more reliable way to view the notebook file online is by using [nbviewer](https://nbviewer.jupyter.org/), which is the official viewer of the Jupyter Notebook project. [Click here](https://nbviewer.jupyter.org/github/metadatacenter/cedar-experiments-curation/blob/master/AAA.ipynb) to open our notebook using nbviewer.\n",
    "\n",
    "The interactive features of our notebook will not work neither from GitHub nor nbviewer. For a fully interactive version of this notebook, you can set up a Jupyter Notebook server locally and start it from the local folder where you cloned the repository. For more information, see [Jupyter's official documentation](https://jupyter.org/install.html). Once your local Jupyter Notebook server is running, go to [http://localhost:8888/](http://localhost:8888/) and click on `AAA.ipynb` to open our notebook. You can also run the notebook on [Binder](https://mybinder.org/) by clicking [here](https://mybinder.org/v2/gh/metadatacenter/AAA)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"s1\"></a>Step 1: Dataset download\n",
    "On Sep 29, 2019, we downloaded the full content of the [NCBI BioSample database](https://www.ncbi.nlm.nih.gov/biosample/) from the [NCBI BioSample FTP repository](https://ftp.ncbi.nih.gov/biosample/) as a .gz file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Download the most recent NCBI biosamples to the workspace\n",
    "import os\n",
    "import urllib.request\n",
    "import scripts.constants as c\n",
    "import scripts.util as util\n",
    "\n",
    "print('Source URL: ' + c.NCBI_DOWNLOAD_URL)\n",
    "if not os.path.exists(c.NCBI_SAMPLES_FOLDER_DEST):\n",
    "    os.makedirs(c.NCBI_SAMPLES_FOLDER_DEST)\n",
    "dest_path = os.path.join(c.NCBI_SAMPLES_FOLDER_DEST, c.NCBI_SAMPLES_FILE_DEST)\n",
    "print('Destination file: ' + dest_path)\n",
    "if os.path.exists(dest_path):\n",
    "    if util.confirm(\"The destination file already exist. Do you want to overwrite it [y/n]?\"):\n",
    "        urllib.request.urlretrieve(c.NCBI_DOWNLOAD_URL, dest_path, reporthook=util.log_progress)\n",
    "else:\n",
    "    urllib.request.urlretrieve(c.NCBI_DOWNLOAD_URL, dest_path, reporthook=util.log_progress)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"s2\"></a>Step 2: Generation of template instances\n",
    "\n",
    "### <a name=\"s2-1\"></a>2.1. Determine relevant attributes and create CEDAR templates\n",
    "\n",
    "We created a CEDAR template with all the attributes defined by the [NCBI BioSample Human Package v1.0](https://submit.ncbi.nlm.nih.gov/biosample/template/?package=Human.1.0&action=definition), which are: *biosample_accession, sample_name, sample_title, bioproject_accession, organism, isolate, age, biomaterial_provider, sex, tissue, cell_line, cell_subtype, cell_type, culture_collection, dev_stage, disease, disease_stage, ethnicity, health_state, karyotype, phenotype, population, race, sample_type, treatment, description*. The template is available in the resources folder at [resources/cedar_template/ncbi_template.json](resources/cedar_template/ncbi_template.json).\n",
    "\n",
    "We focused our analysis on the subset of fields that meet two key requirements: (1) they usually contain values; and (2) they contain categorical values, that is, they represent information about discrete characteristics. We selected 6 fields that met these criteria. These fields are: *sex, tissue, cell line, cell type, disease, and ethnicity*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"s2-2\"></a>2.2. Select samples\n",
    "\n",
    "We filtered the samples based on two criteria:\n",
    "* The sample is from \"Homo sapiens\" (organism=Homo sapiens).\n",
    "* The sample has non-empty values for at least 3 of the 6 fields listed in the previous section.\n",
    "\n",
    "#### <a name=\"s2-2-a\"></a>2.2.a. NCBI BioSample\n",
    "\n",
    "Script used: [step2_filtering.py](scripts/step2_filtering.py). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Filter the NCBI samples\n",
    "%run ./scripts/step2_filtering.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execution results:\n",
    "Finished processing NCBI samples\n",
    "- Total samples processed: 11,625,524\n",
    "- Total samples selected: 262,114\n",
    "\n",
    "The result is an XML file with 262,114 samples ([biosample_filtered.xml](./workspace/samples/filtered/biosample_filtered.xml)). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"s2-3\"></a>2.3. Generate CEDAR instances\n",
    "\n",
    "We transformed the samples obtained from the previous step to CEDAR template instances conforming to [CEDAR's JSON-based Template Model](https://metadatacenter.org/tools-training/outreach/cedar-template-model).\n",
    "\n",
    "Script used: [step3_instance_generation.py](scripts/step3_instance_generation.py): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file: ./workspace/samples/filtered/biosample_filtered.xml\n",
      "Extracting all samples from file (no. samples: 262114)\n",
      "Randomly picking 262114 samples\n",
      "Generating CEDAR instances...\n",
      "No. instances generated: 10000 (4%)\n",
      "No. instances generated: 20000 (8%)\n",
      "No. instances generated: 30000 (11%)\n",
      "No. instances generated: 40000 (15%)\n",
      "No. instances generated: 50000 (19%)\n",
      "No. instances generated: 60000 (23%)\n",
      "No. instances generated: 70000 (27%)\n",
      "No. instances generated: 80000 (31%)\n",
      "No. instances generated: 90000 (34%)\n",
      "No. instances generated: 100000 (38%)\n",
      "No. instances generated: 110000 (42%)\n",
      "No. instances generated: 120000 (46%)\n",
      "No. instances generated: 130000 (50%)\n",
      "No. instances generated: 140000 (53%)\n",
      "No. instances generated: 150000 (57%)\n",
      "No. instances generated: 160000 (61%)\n",
      "No. instances generated: 170000 (65%)\n",
      "No. instances generated: 180000 (69%)\n",
      "No. instances generated: 190000 (72%)\n",
      "No. instances generated: 200000 (76%)\n",
      "No. instances generated: 210000 (80%)\n",
      "No. instances generated: 220000 (84%)\n",
      "No. instances generated: 230000 (88%)\n",
      "No. instances generated: 240000 (92%)\n",
      "No. instances generated: 250000 (95%)\n",
      "No. instances generated: 260000 (99%)\n",
      "Finished\n",
      "CPU times: user 2min 40s, sys: 40.5 s, total: 3min 20s\n",
      "Wall time: 3min 27s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Generate CEDAR instances from NCBI samples\n",
    "%run ./scripts/step3_instance_generation.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"s4\"></a>Step 3: Generation of experimental data sets\n",
    "\n",
    "When we generated the CEDAR instances (step 2.3), we partitioned the resulting instances for each database (NCBI, EBI) into two datasets, with 85% of the data for training and the remaining 15% for testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"s5\"></a>Step 5: Training\n",
    "\n",
    "We mined association rules from the training sets to discover the hidden relationships between metadata fields. We extracted the rules using a local installation of the CEDAR Workbench. We set up the Value Recommender service to read the instance files from a local folder by updating the [Constants.java](https://github.com/metadatacenter/cedar-valuerecommender-server/blob/master/cedar-valuerecommender-server-core/src/main/java/org/metadatacenter/intelligentauthoring/valuerecommender/util/Constants.java) file as follows:\n",
    "\n",
    "```Java\n",
    "READ_INSTANCES_FROM_CEDAR = false // Read training instances from a local folder\n",
    "```\n",
    "\n",
    "Uupdate the variable `CEDAR_INSTANCES_PATH` with the full path of the corresponding training set: `workspace/cedar_instances/ncbi_cedar_instances/training`\n",
    "\n",
    "Internally, CEDAR's Value Recommender uses a [WEKA's implementation of the Apriori algorithm](https://www.cs.waikato.ac.nz/ml/weka/).\n",
    "\n",
    "Compile the `cedar-valuerecommender-server` project and start it locally. You can trigger the rule generation process from the command line using the following curl command:\n",
    "```\n",
    "curl --request POST \\\n",
    "  --url https://valuerecommender.metadatacenter.orgx/command/generate-rules/<TEMPLATE_ID> \\\n",
    "  --header 'authorization: apiKey <CEDAR_ADMIN_API_KEY>' \\\n",
    "  --header 'content-type: application/json' \\\n",
    "  --data '{}'\n",
    "```\n",
    "\n",
    "where `CEDAR_ADMIN_API_KEY` is the API key of the *cedar-admin* user in your local CEDAR system, and `TEMPLATE_ID` is the local identifier of the template that you want to extract rules for."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------end of document-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "### Rules generation\n",
    "Take the text instances from the arm experiment and run the server with the READ_INSTANCES_FROM_CEDAR constant set to false, and with the right path for the constant CEDAR_INSTANCES_PATH.\n",
    "\n",
    "### Rules ingestion\n",
    "\n",
    "1. Delete the existing rules in Elasticsearch: `cedarat rules-regenerateIndex`\n",
    "2. Download the NCBI (text-based) rules: https://drive.google.com/file/d/1ngCTGf4To1NZ1puRsB3aaCvtZIAERktY/view?usp=sharing\n",
    "3. Extract the rules to a local folder. Then, import the 30,295 rules using:\n",
    "    \n",
    "    `elasticdump --input=./ncbi-text-rules-data.json --output=http://localhost:9200/cedar-rules --type=data`\n",
    "    \n",
    "    \n",
    "4. Check that the rules have been imported correctly using Kibana (http://localhost:5601):\n",
    "    \n",
    "    `GET cedar-rules/_search`\n",
    "    \n",
    "5. Script used to generate recommendations: ...    \n",
    "\n",
    "Notes:\n",
    "- I will use the rules generated from the training set. Then, I will use the test set to evaluate the curation process.\n",
    "\n",
    "Other possible topics to mention:\n",
    "- Configuration settings\n",
    "\n",
    "\n",
    "## References\n",
    "\n",
    "* ARM-evaluation Jupyter notebook: https://nbviewer.jupyter.org/github/metadatacenter/cedar-experiments-valuerecommender2019/blob/master/ValueRecommenderEvaluation.ipynb#s5-results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elasticdump --input=./ncbi-text-data.json --output=http://localhost:9200/cedar-rules --type=data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elasticdump --input=./ncbi-text-data.json --output=http://localhost:9200/cedar-rules --type=data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
